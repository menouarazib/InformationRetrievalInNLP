{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 21179,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 15189
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "PyGemma-Assistant",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menouarazib/InformationRetrievalInNLP/blob/master/PyGemma_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyGemma Assistant**\n",
        "\n",
        "In this notebook, we will demonstrate the creation of an AI Assistant designed to answer Python-related questions. The responses will be generated by a model named `pygemma`. `pygemma` is a fine-tuned version of the [Gemma 2b model](https://www.kaggle.com/models/google/gemma/) and has been trained on a publicly available Python dataset. It is specifically designed to assist developers by providing answers to common questions about the Python programming language, making `pygemma` an invaluable resource for developers seeking Python-related assistance on Kaggle.\n",
        "\n",
        "More details about `pygemma` can be found [on Kaggle](https://www.kaggle.com/models/menouarazib/pygemma-2b).\n",
        "\n",
        "To create an AI Assistant using `pygemma`, we need to follow these major steps:\n",
        "\n",
        "1. **Install the required libraries**\n",
        "2. **Load `pygemma` from its Kaggle location**\n",
        "3. **Create the AI assistant which utilizes `pygemma` to generate responses**\n",
        "4. **Test the AI assistant with some questions in Python**"
      ],
      "metadata": {
        "id": "2fxMddSd374r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Install the required libraries**\n",
        "\n",
        "The `transformers` library is used to load `pygemma` from its Kaggle location.\n",
        "\n",
        "The `accelerate` library is a great tool for running your model on a single or multiple GPUs. It simplifies launching distributed and mixed-precision inference.\n",
        "\n",
        "The `rich` library in Python is used for rich text and beautiful formatting to the terminal. It can colorize console output, create tables, render markdown, syntax highlight source code, and more. It's a great tool for making your console output more readable and visually appealing."
      ],
      "metadata": {
        "id": "3eHfoaNt374r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip install -q rich"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:10:57.046151Z",
          "iopub.execute_input": "2024-03-28T18:10:57.046522Z",
          "iopub.status.idle": "2024-03-28T18:11:34.330435Z",
          "shell.execute_reply.started": "2024-03-28T18:10:57.046494Z",
          "shell.execute_reply": "2024-03-28T18:11:34.329346Z"
        },
        "trusted": true,
        "id": "siCBMqyO374s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Load `pygemma` from its Kaggle location**\n",
        "\n",
        "1. **Import necessary libraries**: The `transformers` library is imported for its `AutoTokenizer` and `AutoModelForCausalLM` classes. The `torch` library is imported for its CUDA and tensor functionalities.\n",
        "\n",
        "2. **Check CUDA availability**: The code checks if CUDA is available. If it is, it sets the device to 'cuda'. It also checks if the CUDA device has a new architecture (compute capability >= 8).\n",
        "\n",
        "3. **Set data type**: The default data type for tensors is set to `float32`. If the device is 'cuda', the data type is changed to `float16`. If the CUDA device has a new architecture, the data type is further changed to `bfloat16`.\n",
        "\n",
        "4. **Load pretrained model and tokenizer**: The path to the pretrained model is defined. The tokenizer and the model for causal language modeling are loaded from the pretrained model. The model is automatically mapped to the available device, and the tensor data type is set according to the device. The model is loaded with settings optimized for low CPU memory usage, and only local files are used (no downloads)."
      ],
      "metadata": {
        "id": "NxIZUjR7374s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize a flag for CUDA's new architecture\n",
        "cuda_new_architecture = False\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available.\")\n",
        "    # Check if the CUDA device has a new architecture (compute capability >= 8)\n",
        "    cuda_new_architecture = torch.cuda.get_device_capability()[0] >= 8\n",
        "    # Set the device to 'cuda'\n",
        "    device = 'cuda'\n",
        "    # If the CUDA device does not have a new architecture, print an empty line\n",
        "    if not cuda_new_architecture:\n",
        "        print('The CUDA device has a new architecture (compute capability >= 8)')\n",
        "else:\n",
        "    # If CUDA is not available, set the device to 'cpu'\n",
        "    device = 'cpu'\n",
        "\n",
        "# Set the default torch dtype to float32\n",
        "torch_dtype = torch.float32\n",
        "\n",
        "# If the device is 'cuda', change the torch dtype to float16\n",
        "if device == 'cuda':\n",
        "    torch_dtype = torch.float16\n",
        "    # If the CUDA device has a new architecture, change the torch dtype to bfloat16\n",
        "    if cuda_new_architecture:\n",
        "        torch_dtype = torch.bfloat16\n",
        "\n",
        "# Define the path to the pretrained model\n",
        "kaggle_model = \"/kaggle/input/pygemma-2b/transformers/v1/2\"\n",
        "\n",
        "# Load the tokenizer from the pretrained model\n",
        "pygemma_tokenizer = AutoTokenizer.from_pretrained(kaggle_model, local_files_only=True)\n",
        "# Load the model for causal language modeling from the pretrained model\n",
        "# Set the device map to 'auto' to automatically map the model to the available device\n",
        "# Set the torch dtype according to the device\n",
        "# Set local_files_only to True to only use local files and not download anything\n",
        "# Set low_cpu_mem_usage to True to optimize for low CPU memory usage\n",
        "pygemma_model = AutoModelForCausalLM.from_pretrained(kaggle_model, device_map=\"auto\", torch_dtype=torch_dtype,\n",
        "                                                     local_files_only=True, low_cpu_mem_usage=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:11:34.332307Z",
          "iopub.execute_input": "2024-03-28T18:11:34.332619Z",
          "iopub.status.idle": "2024-03-28T18:12:40.898448Z",
          "shell.execute_reply.started": "2024-03-28T18:11:34.33259Z",
          "shell.execute_reply": "2024-03-28T18:12:40.89769Z"
        },
        "trusted": true,
        "id": "Nsy0dD9D374u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Create the AI assistant which utilizes `pygemma` to generate responses**"
      ],
      "metadata": {
        "id": "zca7fNB5374v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We create utility classes for PyGemma Assistant:"
      ],
      "metadata": {
        "id": "IT0AKSQy374w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria\n",
        "import torch\n",
        "from typing import List\n",
        "\n",
        "class StoppingCriteriaCustom(StoppingCriteria):\n",
        "    \"\"\"\n",
        "    Custom stopping criteria for text generation to prevent the generation of unnecessary content.\n",
        "\n",
        "    Args:\n",
        "        input_len (int): The length of the input sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_len: int, stops_ids: List[int]):\n",
        "        super().__init__()\n",
        "        self.input_len = input_len\n",
        "        # If these tokens are encountered during generation, the process should stop.\n",
        "        # This is to prevent the generation of unnecessary content.\n",
        "        # ids = \" any further questions.\"\n",
        "        self.stops_ids = stops_ids\n",
        "\n",
        "    def _check_sequence(self, tensor: torch.LongTensor):\n",
        "        tensor_list = tensor.tolist()[self.input_len:]\n",
        "        sequence_length = len(self.stops_ids)\n",
        "        try:\n",
        "            start_index = tensor_list.index(self.stops_ids[0])\n",
        "        except ValueError:\n",
        "            return False\n",
        "        for i in range(start_index, len(tensor_list) - sequence_length + 1):\n",
        "            if tensor_list[i:i + sequence_length] == self.stops_ids:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
        "        \"\"\"\n",
        "        Checks if a specific sequence appears in the generated text after the input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): The IDs of the tokens in the generated text.\n",
        "            scores (torch.FloatTensor): The scores of the tokens in the generated text.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the specific sequence is found after the input sequence, False otherwise.\n",
        "        \"\"\"\n",
        "        return self._check_sequence(input_ids[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:40.899629Z",
          "iopub.execute_input": "2024-03-28T18:12:40.900165Z",
          "iopub.status.idle": "2024-03-28T18:12:40.910295Z",
          "shell.execute_reply.started": "2024-03-28T18:12:40.900128Z",
          "shell.execute_reply": "2024-03-28T18:12:40.909285Z"
        },
        "trusted": true,
        "id": "Kt_ou4zn374w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteriaList\n",
        "import torch\n",
        "from transformers.models.gemma.modeling_gemma import GemmaForCausalLM\n",
        "from transformers.models.gemma.tokenization_gemma_fast import GemmaTokenizerFast\n",
        "\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class PyGemmaResponseGenerator:\n",
        "    \"\"\"\n",
        "    A class that uses a PyGemma model to generate responses to user queries in Python.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pygemma: GemmaForCausalLM, pygemma_tokenizer: GemmaTokenizerFast, available_device: str,\n",
        "                 do_sample: bool = False,\n",
        "                 temperature: float = 0.2, max_new_tokens: int = 768, top_k: int = 50, top_p: float = 0.99):\n",
        "        \"\"\"\n",
        "        Initializes the PyGemmaResponseGenerator.\n",
        "\n",
        "        Args:\n",
        "            pygemma (GemmaForCausalLM): The PyGemma model to use for generating replies.\n",
        "            pygemma_tokenizer (GemmaTokenizerFast): The PyGemma tokenizer to use for preprocessing text.\n",
        "            available_device (str): The device to be used for computations ('cuda' if CUDA is available, otherwise 'cpu').\n",
        "            do_sample (bool): Controls the type of decoding strategy used for text generation.\n",
        "                If set to False, the model uses greedy decoding that picks the token with the highest probability\n",
        "                as the next token.\n",
        "                If set to True, the model uses sampling, meaning it samples the next token from the output\n",
        "                distribution. The choice of decoding strategy can significantly impact the generated text. Greedy decoding\n",
        "                tends to generate repetitive and overly confident text, while multinomial sampling can produce more diverse\n",
        "                and realistic text, albeit at the risk of making more mistakes.\n",
        "            temperature (float, optional): The temperature to use when sampling from the model's output distribution.\n",
        "                Defaults to 0.7.\n",
        "            max_new_tokens (int, optional): The maximum number of tokens to generate. Defaults to 512.\n",
        "            top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "             Defaults to 50.\n",
        "            top_p (float, optional): Only the most probable tokens with probabilities that add up to top_p or higher are\n",
        "             kept for generation. Defaults to 0.95.\n",
        "        \"\"\"\n",
        "        # Initialize the PyGemma model and tokenizer\n",
        "        self.pygemma = pygemma\n",
        "        self.pygemma_tokenizer = pygemma_tokenizer\n",
        "\n",
        "        # Set the device for computations\n",
        "        self.available_device = available_device\n",
        "\n",
        "        # Set the parameters for text generation\n",
        "        self.do_sample = do_sample\n",
        "        self.temperature = temperature\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "        self.top_k = top_k\n",
        "        self.top_p = top_p\n",
        "\n",
        "        # Set the system prompt for the PyGemma model\n",
        "        self.system_prompt = \"\"\"Welcome to PyGemma, your AI-powered Python assistant. I'm here to assist you with\n",
        "        common questions about the Python programming language. Let's dive into Python!\"\"\"\n",
        "\n",
        "        # If these tokens are encountered during generation, the process should stop.\n",
        "        # This is to prevent the generation of unnecessary content.\n",
        "        # ids = \" any further questions.\"\n",
        "        self.stops_ids = [1089, 4024, 3920, 235265]\n",
        "\n",
        "        # The model's maximum input length used during the finetuning process.\n",
        "        self.max_input_token_length = 3072\n",
        "\n",
        "    def _add_prompt_to_chat(self, chat_history: List[Tuple[str, str]], prompt: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Adds the new user prompt to chat history and format them into the format required by PyGEMMA.\n",
        "\n",
        "        Args:\n",
        "            chat_history (List[Tuple[str, str]]): The chat history between the user and the assistant.\n",
        "            prompt (str): The new user prompt to which a reply is needed.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict[str, str]]: The formatted conversation.\n",
        "        \"\"\"\n",
        "        conversation = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "        for user, assistant in chat_history:\n",
        "            conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
        "\n",
        "        conversation.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        return conversation\n",
        "\n",
        "    def _tokenize(self, conversation: List[Dict[str, str]]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tokenizes the conversation.\n",
        "\n",
        "        Args:\n",
        "            conversation (List[Dict[str, str]]): The conversation to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The tokenized conversation.\n",
        "        \"\"\"\n",
        "\n",
        "        input_ids = self.pygemma_tokenizer.apply_chat_template(conversation, return_tensors=\"pt\",\n",
        "                                                               add_generation_prompt=True).to(self.available_device)\n",
        "\n",
        "        # If the conversation is too long, trim it to fit within model's maximum input length\n",
        "        if input_ids.shape[1] > self.max_input_token_length:\n",
        "            input_ids = input_ids[:, -self.max_input_token_length:]\n",
        "            print(f\"Trimmed input from conversation as it exceeded {self.max_input_token_length} tokens.\")\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "    def generate_response(self, chat_history: List[Tuple[str, str]], prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generates a response to the given prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The prompt to which a reply is needed.\n",
        "            chat_history (List[Tuple[str, str]]): The chat history between the user and the assistant.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated reply.\n",
        "        \"\"\"\n",
        "        # Format the chat history and the new user prompt\n",
        "        conversation = self._add_prompt_to_chat(chat_history=chat_history, prompt=prompt)\n",
        "\n",
        "        # Tokenize the conversation\n",
        "        input_ids = self._tokenize(conversation=conversation)\n",
        "\n",
        "        stopping_criteria = StoppingCriteriaList([StoppingCriteriaCustom(len(input_ids[0]), self.stops_ids)])\n",
        "\n",
        "        # Generate a reply\n",
        "        if self.do_sample:\n",
        "            outputs = self.pygemma.generate(input_ids, max_new_tokens=self.max_new_tokens, do_sample=self.do_sample,\n",
        "                                            temperature=self.temperature,\n",
        "                                            top_p=self.top_p, top_k=self.top_k, stopping_criteria=stopping_criteria)\n",
        "        else:\n",
        "            outputs = self.pygemma.generate(input_ids, max_new_tokens=self.max_new_tokens,\n",
        "                                            stopping_criteria=stopping_criteria, forced_eos_token_id=self.stops_ids)\n",
        "\n",
        "        # The 'outputs' tensor contains both the input tokens and the newly generated tokens.\n",
        "        # To get only the generated tokens, we slice the tensor to exclude the input tokens.\n",
        "        # The slicing operation 'outputs[0][input_ids.shape[-1]:]' removes the input tokens from the output.\n",
        "        encoded_reply = outputs[0][input_ids.shape[-1]:]\n",
        "\n",
        "        # Decode the generated tokens to convert them back into text.\n",
        "        # The 'decode' method takes the token ids and converts them into human-readable text.\n",
        "        # 'skip_special_tokens=True' removes any special tokens that were added during tokenization.\n",
        "        reply = self.pygemma_tokenizer.decode(encoded_reply, skip_special_tokens=True)\n",
        "\n",
        "        # Return the generated reply\n",
        "        return reply"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:40.912909Z",
          "iopub.execute_input": "2024-03-28T18:12:40.913187Z",
          "iopub.status.idle": "2024-03-28T18:12:40.934148Z",
          "shell.execute_reply.started": "2024-03-28T18:12:40.913164Z",
          "shell.execute_reply": "2024-03-28T18:12:40.933265Z"
        },
        "trusted": true,
        "id": "dA4-0Mig374x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **We create a class that represents the PyGemma AI assistant:**"
      ],
      "metadata": {
        "id": "p_srP9Vf374z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyGemmaAssistant:\n",
        "    \"\"\"A class which represents PyGemma AI assistant.\n",
        "\n",
        "    Attributes:\n",
        "        _chat_history (list): A list to store the chat history.\n",
        "        pygemma_response_generator (PyGemmaResponseGenerator): An object of PyGemmaResponseGenerator.\n",
        "        _use_chat_history (bool): A flag indicating whether to use chat history.\n",
        "\n",
        "    Args:\n",
        "        pygemma_response_generator (PyGemmaResponseGenerator): An object of PyGemmaResponseGenerator.\n",
        "        use_chat_history (bool, optional): A flag indicating whether to use chat history. Defaults to False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pygemma_response_generator: PyGemmaResponseGenerator, use_chat_history: bool = False):\n",
        "        self._chat_history = []\n",
        "        self.pygemma_response_generator = pygemma_response_generator\n",
        "        self._use_chat_history = use_chat_history\n",
        "\n",
        "    def use_chat_history(self, use_chat: bool):\n",
        "        self._chat_history = []\n",
        "        self._use_chat_history = use_chat\n",
        "\n",
        "    def ask(self, message: str) -> str:\n",
        "        \"\"\"Generates a response to a given message.\n",
        "\n",
        "        If use_chat_history is True, the existing chat history is used for generating the response.\n",
        "        The user's message and the assistant's response are appended to the chat history.\n",
        "\n",
        "        Args:\n",
        "            message (str): The user's message.\n",
        "\n",
        "        Returns:\n",
        "            str: The assistant's response.\n",
        "        \"\"\"\n",
        "        chat_history_ = (self._chat_history if self._use_chat_history else [])\n",
        "        assistant_answer = self.pygemma_response_generator.generate_response(chat_history=chat_history_, prompt=message)\n",
        "        self._chat_history.append((message, assistant_answer))\n",
        "        return assistant_answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:40.935107Z",
          "iopub.execute_input": "2024-03-28T18:12:40.935599Z",
          "iopub.status.idle": "2024-03-28T18:12:40.951058Z",
          "shell.execute_reply.started": "2024-03-28T18:12:40.935574Z",
          "shell.execute_reply": "2024-03-28T18:12:40.950123Z"
        },
        "trusted": true,
        "id": "gKwlhfhU3740"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Test the PyGemma Assistant with some questions in Python**"
      ],
      "metadata": {
        "id": "fFfr2ovB3742"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We instantiate the objects\n",
        "pygemma_response_generator = PyGemmaResponseGenerator(pygemma=pygemma_model, pygemma_tokenizer=pygemma_tokenizer, available_device=device)\n",
        "pygemma_assistant = PyGemmaAssistant(pygemma_response_generator)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:40.952104Z",
          "iopub.execute_input": "2024-03-28T18:12:40.952441Z",
          "iopub.status.idle": "2024-03-28T18:12:40.965467Z",
          "shell.execute_reply.started": "2024-03-28T18:12:40.952411Z",
          "shell.execute_reply": "2024-03-28T18:12:40.964614Z"
        },
        "trusted": true,
        "id": "g9UVw1_-3743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "response = pygemma_assistant.ask(\"What is the difference between a list and a tuple in Python?\")\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:40.966487Z",
          "iopub.execute_input": "2024-03-28T18:12:40.96675Z",
          "iopub.status.idle": "2024-03-28T18:12:48.644174Z",
          "shell.execute_reply.started": "2024-03-28T18:12:40.966728Z",
          "shell.execute_reply": "2024-03-28T18:12:48.643268Z"
        },
        "trusted": true,
        "id": "hkv7Zii73743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "response = pygemma_assistant.ask(\"How do you handle exceptions in Python?\")\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:48.645433Z",
          "iopub.execute_input": "2024-03-28T18:12:48.645949Z",
          "iopub.status.idle": "2024-03-28T18:12:56.602113Z",
          "shell.execute_reply.started": "2024-03-28T18:12:48.645914Z",
          "shell.execute_reply": "2024-03-28T18:12:56.601044Z"
        },
        "trusted": true,
        "id": "3MBJ4ypH3743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "response = pygemma_assistant.ask(\"What are decorators in Python?\")\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:12:56.603805Z",
          "iopub.execute_input": "2024-03-28T18:12:56.604177Z",
          "iopub.status.idle": "2024-03-28T18:13:06.639545Z",
          "shell.execute_reply.started": "2024-03-28T18:12:56.604143Z",
          "shell.execute_reply": "2024-03-28T18:13:06.638537Z"
        },
        "trusted": true,
        "id": "pbesvtJk3743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "response = pygemma_assistant.ask(\"What is a lambda function in Python?\")\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:06.642703Z",
          "iopub.execute_input": "2024-03-28T18:13:06.643009Z",
          "iopub.status.idle": "2024-03-28T18:13:14.733521Z",
          "shell.execute_reply.started": "2024-03-28T18:13:06.642979Z",
          "shell.execute_reply": "2024-03-28T18:13:14.732462Z"
        },
        "trusted": true,
        "id": "ffI9jzwP3744"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "question = \"\"\"Can you write a Python function that takes a string as input and returns a dictionary where the keys are the\n",
        "unique characters in the string and the values are the number of times each character appears in the string?\"\"\"\n",
        "\n",
        "response = pygemma_assistant.ask(question)\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:14.73476Z",
          "iopub.execute_input": "2024-03-28T18:13:14.735642Z",
          "iopub.status.idle": "2024-03-28T18:13:24.064119Z",
          "shell.execute_reply.started": "2024-03-28T18:13:14.735608Z",
          "shell.execute_reply": "2024-03-28T18:13:24.063032Z"
        },
        "trusted": true,
        "id": "rgiMUy4x3744"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "question = \"\"\"Can you write a Python function that takes a list of integers as input, and returns a list of those\n",
        "integers sorted in descending order, but with all the odd numbers at the front of the list and even numbers at the\n",
        "back? The odd numbers should be sorted in descending order, and the even numbers should be sorted in ascending order.\"\"\"\n",
        "\n",
        "response = pygemma_assistant.ask(question)\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:24.065316Z",
          "iopub.execute_input": "2024-03-28T18:13:24.065594Z",
          "iopub.status.idle": "2024-03-28T18:13:33.044052Z",
          "shell.execute_reply.started": "2024-03-28T18:13:24.065571Z",
          "shell.execute_reply": "2024-03-28T18:13:33.043076Z"
        },
        "trusted": true,
        "id": "1xk1NYAJ3744"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enable To Use Chat History**"
      ],
      "metadata": {
        "id": "sLn10egu3744"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pygemma_assistant.use_chat_history(True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:33.045152Z",
          "iopub.execute_input": "2024-03-28T18:13:33.04542Z",
          "iopub.status.idle": "2024-03-28T18:13:33.049544Z",
          "shell.execute_reply.started": "2024-03-28T18:13:33.045398Z",
          "shell.execute_reply": "2024-03-28T18:13:33.048668Z"
        },
        "trusted": true,
        "id": "6lq6B1HO3745"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.markdown import Markdown\n",
        "\n",
        "question1 = \"\"\"Can you write a Python function that takes a list of numbers as input and returns the sum of all the numbers in the list?\"\"\"\n",
        "\n",
        "response = pygemma_assistant.ask(question1)\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:33.050708Z",
          "iopub.execute_input": "2024-03-28T18:13:33.051036Z",
          "iopub.status.idle": "2024-03-28T18:13:39.805769Z",
          "shell.execute_reply.started": "2024-03-28T18:13:33.050999Z",
          "shell.execute_reply": "2024-03-28T18:13:39.804752Z"
        },
        "trusted": true,
        "id": "KDaVvcg63745"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"\"\"After getting the function for the sum, can you modify it to return the average of the numbers in the list instead?\"\"\"\n",
        "\n",
        "response = pygemma_assistant.ask(question2)\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:39.806882Z",
          "iopub.execute_input": "2024-03-28T18:13:39.807138Z",
          "iopub.status.idle": "2024-03-28T18:13:47.191587Z",
          "shell.execute_reply.started": "2024-03-28T18:13:39.807116Z",
          "shell.execute_reply": "2024-03-28T18:13:47.190661Z"
        },
        "trusted": true,
        "id": "KWy-kb2C3745"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = pygemma_assistant.ask(\"Thank you\")\n",
        "Markdown(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T18:13:47.192703Z",
          "iopub.execute_input": "2024-03-28T18:13:47.19297Z",
          "iopub.status.idle": "2024-03-28T18:13:48.096273Z",
          "shell.execute_reply.started": "2024-03-28T18:13:47.192947Z",
          "shell.execute_reply": "2024-03-28T18:13:48.095205Z"
        },
        "trusted": true,
        "id": "UhNnEZc93745"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}